{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_CSS_CODE_MODELLING.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlcnr-xOYWwg",
        "colab_type": "code",
        "outputId": "296b3634-dc5f-4357-d6ab-064facd16050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "!pip install -q -U tensorflow>=2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-71q8GaLle",
        "colab_type": "text"
      },
      "source": [
        "Read in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDOizYvwZgGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open(\"css.txt\", \"rb\").read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu2Kb7v3aIBj",
        "colab_type": "code",
        "outputId": "fb18528d-0bba-4ef5-eb03-25272a3a4f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print ('Length of text: {} characters'.format(len(corpus)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 3419905 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SueFLkaTaWSa",
        "colab_type": "code",
        "outputId": "14f86709-1765-4a21-92b2-aca5276c04f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(corpus[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/*------------------style-----------------*/\n",
            "\n",
            "/*\n",
            "Template Name: Apex App\t\n",
            "Author: MarkUps\n",
            "Author URI: http://www.markups.io/\n",
            "Version: 1.0\n",
            "*/\n",
            "\n",
            "\n",
            "/* Table of Content\n",
            "==================================================\n",
            "#BASIC TYPOGRAPHY\n",
            "#HEADER\t\n",
            "#MENU \n",
            "#F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezuGJh1Bb4Ey",
        "colab_type": "code",
        "outputId": "2130f48f-4089-4dde-8675-95ebbabe8c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(corpus))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135W-t1XdsTZ",
        "colab_type": "text"
      },
      "source": [
        "Vectorize text- map all unique characters to an integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LJ9rDgDdxwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koYuLG-Id6QW",
        "colab_type": "code",
        "outputId": "54da8dec-6b36-4387-c180-b68f1087cf4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\t':   0,\n",
            "  '\\n':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '$' :   6,\n",
            "  '%' :   7,\n",
            "  '&' :   8,\n",
            "  \"'\" :   9,\n",
            "  '(' :  10,\n",
            "  ')' :  11,\n",
            "  '*' :  12,\n",
            "  '+' :  13,\n",
            "  ',' :  14,\n",
            "  '-' :  15,\n",
            "  '.' :  16,\n",
            "  '/' :  17,\n",
            "  '0' :  18,\n",
            "  '1' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0shl94DeDVL",
        "colab_type": "code",
        "outputId": "5c2bb329-7104-4667-b9a7-97a869afb950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(corpus[:25]), text_as_int[:25]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/*------------------style' ---- characters mapped to int ---- > [17 12 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 84 85 90 77\n",
            " 70]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBBAFyuieKV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 50\n",
        "examples_per_epoch = len(corpus)//(seq_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htu3kiLNgiLi",
        "colab_type": "code",
        "outputId": "bae4c315-d421-448b-80a3-e48c2bc0cca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#convert stream of text into character indices\n",
        "# Create training examples / targets\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "*\n",
            "-\n",
            "-\n",
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpyy8YhvkYjX",
        "colab_type": "text"
      },
      "source": [
        "Divide characters into batch size of specific lenght"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgfZIbxugzQB",
        "colab_type": "code",
        "outputId": "33c6a485-4dc7-458f-9c80-12fe0cd2fa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/*------------------style-----------------*/\\n\\n/*\\nTe'\n",
            "'mplate Name: Apex App\\t\\nAuthor: MarkUps\\nAuthor URI: '\n",
            "'http://www.markups.io/\\nVersion: 1.0\\n*/\\n\\n\\n/* Table o'\n",
            "'f Content\\n========================================='\n",
            "'=========\\n#BASIC TYPOGRAPHY\\n#HEADER\\t\\n#MENU \\n#FEATUR'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVPncLVmxK2",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt0yygymlzKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLbEaQ8Vm72J",
        "colab_type": "text"
      },
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2uUHSdqm8Zh",
        "colab_type": "code",
        "outputId": "0cd0aa90-091c-43db-abf9-88d3a85b2460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  '/*------------------style-----------------*/\\n\\n/*\\nT'\n",
            "Target data: '*------------------style-----------------*/\\n\\n/*\\nTe'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhwUbpJXn9yn",
        "colab_type": "text"
      },
      "source": [
        "The input of the RNN will start from '<' and predict '!' in the next time step till the final predicted output is 't' the last character in the sequence before it moves to the next sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGg-pcedsFns",
        "colab_type": "code",
        "outputId": "60145046-cb83-441d-c911-0a08a2a034ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 17 ('/')\n",
            "  expected output: 12 ('*')\n",
            "Step    1\n",
            "  input: 12 ('*')\n",
            "  expected output: 15 ('-')\n",
            "Step    2\n",
            "  input: 15 ('-')\n",
            "  expected output: 15 ('-')\n",
            "Step    3\n",
            "  input: 15 ('-')\n",
            "  expected output: 15 ('-')\n",
            "Step    4\n",
            "  input: 15 ('-')\n",
            "  expected output: 15 ('-')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hotbjJ7fv1NY",
        "colab_type": "code",
        "outputId": "ccf72160-864b-4d6a-c7fe-04ffcfc55632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 98\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((98, 50), (98, 50)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diyWFjQnv1ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Batch size is usually the size of your vocabulary because you will want to maintain a reasonable proportional embedding distance between your vocab\n",
        "\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY8Aig6U4Ls4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdJPZIYW4RiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrALEylv4gzf",
        "colab_type": "code",
        "outputId": "c21f25c3-ab10-4836-ee05-ac3d7958731e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(98, 50, 99) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPt6pdoJ6d7o",
        "colab_type": "code",
        "outputId": "68005efc-be33-4027-8205-93da02cde453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (98, None, 256)           25344     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (98, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (98, None, 99)            101475    \n",
            "=================================================================\n",
            "Total params: 4,065,123\n",
            "Trainable params: 4,065,123\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfq7PJa97pc9",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5nFttwc7rBg",
        "colab_type": "code",
        "outputId": "dd51959e-565d-466d-c988-84f90b9d3137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([69, 63, 20, 50,  0, 70, 52, 95,  8, 36, 41, 70, 79, 76, 46, 80, 37,\n",
              "       58, 12, 97, 32, 67, 29, 26, 36, 85, 97, 79, 30, 39, 81, 54, 57, 79,\n",
              "       87, 66, 88, 35, 46, 83,  1,  1, 70, 33, 90, 85, 43, 38, 92, 44])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE2glKqk8l5p",
        "colab_type": "text"
      },
      "source": [
        "Checking the sample text generated by this untrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD5YeymL765X",
        "colab_type": "code",
        "outputId": "423291aa-7561-45f2-e909-754d589e3b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'x 0 175px;\\n}\\n\\nheader.header .logo {\\n    margin-top'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'd]2P\\teR~&BGenkLoCX*•>b;8Bt•n<EpTWnvawALr\\n\\ne?ytID{J'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M1ooepH87Tq",
        "colab_type": "text"
      },
      "source": [
        "Attach an optimizer, and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcFIrNua8s0a",
        "colab_type": "code",
        "outputId": "225106f9-35af-460b-b869-50402d2b8b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (98, 50, 99)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.5953155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkkvXbaX_wN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClVv7IPhDUqo",
        "colab_type": "text"
      },
      "source": [
        "Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek1oJajA_xB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaegGaZ3FVeU",
        "colab_type": "code",
        "outputId": "fd61254d-b3fc-4b8b-fda4-176f520e9905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "684/684 [==============================] - 2290s 3s/step - loss: 1.3685\n",
            "Epoch 2/10\n",
            "414/684 [=================>............] - ETA: 14:14 - loss: 0.7530"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN-dX1FpjZz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhZNnkokE43z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3vUrZ-1QgzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBoM5iEDQy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.5\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3KL_pKSREVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u\"<!DOCTYPE html> \"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}