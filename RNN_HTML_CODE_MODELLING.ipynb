{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN HTML CODE MODELLING.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlcnr-xOYWwg",
        "colab_type": "code",
        "outputId": "ae1c6d2b-f85b-496c-b574-b18ef5fd94f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "!pip install -q -U tensorflow>=2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-71q8GaLle",
        "colab_type": "text"
      },
      "source": [
        "Read in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDOizYvwZgGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open(\"corpus.txt\", \"rb\").read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu2Kb7v3aIBj",
        "colab_type": "code",
        "outputId": "fb4b264f-1bb0-4444-fc24-5a6511acb2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print ('Length of text: {} characters'.format(len(corpus)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2782185 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SueFLkaTaWSa",
        "colab_type": "code",
        "outputId": "9aa79b49-d20a-4436-ba8a-8aa052d7115d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(corpus[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "  <head>\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <!-- The above 3 meta tags *must* come first i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezuGJh1Bb4Ey",
        "colab_type": "code",
        "outputId": "520f9247-1017-4feb-d26f-b125a97d2609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(corpus))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135W-t1XdsTZ",
        "colab_type": "text"
      },
      "source": [
        "Vectorize text- map all unique characters to an integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LJ9rDgDdxwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koYuLG-Id6QW",
        "colab_type": "code",
        "outputId": "34876b32-f34f-4ac6-f9b1-ccc8bc89da2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\t':   0,\n",
            "  '\\n':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '$' :   6,\n",
            "  '%' :   7,\n",
            "  '&' :   8,\n",
            "  \"'\" :   9,\n",
            "  '(' :  10,\n",
            "  ')' :  11,\n",
            "  '*' :  12,\n",
            "  '+' :  13,\n",
            "  ',' :  14,\n",
            "  '-' :  15,\n",
            "  '.' :  16,\n",
            "  '/' :  17,\n",
            "  '0' :  18,\n",
            "  '1' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0shl94DeDVL",
        "colab_type": "code",
        "outputId": "87b0b762-3719-421b-e746-68c9e6779a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(corpus[:25]), text_as_int[:25]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'<!DOCTYPE html>\\n<html lan' ---- characters mapped to int ---- > [30  3 38 49 37 54 59 50 39  2 73 85 78 77 32  1 30 73 85 78 77  2 77 66\n",
            " 79]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBBAFyuieKV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 50\n",
        "examples_per_epoch = len(corpus)//(seq_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htu3kiLNgiLi",
        "colab_type": "code",
        "outputId": "904390df-b4d5-455e-b3d8-630c2bdd15e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#convert stream of text into character indices\n",
        "# Create training examples / targets\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<\n",
            "!\n",
            "D\n",
            "O\n",
            "C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpyy8YhvkYjX",
        "colab_type": "text"
      },
      "source": [
        "Divide characters into batch size of specific lenght"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgfZIbxugzQB",
        "colab_type": "code",
        "outputId": "e8bed362-6c7e-419a-e3a6-5e55b83804cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta'\n",
            "' charset=\"utf-8\">\\n    <meta http-equiv=\"X-UA-Compat'\n",
            "'ible\" content=\"IE=edge\">\\n    <meta name=\"viewport\" '\n",
            "'content=\"width=device-width, initial-scale=1\">\\n    '\n",
            "'<!-- The above 3 meta tags *must* come first in the'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVPncLVmxK2",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt0yygymlzKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLbEaQ8Vm72J",
        "colab_type": "text"
      },
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2uUHSdqm8Zh",
        "colab_type": "code",
        "outputId": "33860dc4-cffc-4001-9b20-19648c9ef087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  '<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <met'\n",
            "Target data: '!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhwUbpJXn9yn",
        "colab_type": "text"
      },
      "source": [
        "The input of the RNN will start from '<' and predict '!' in the next time step till the final predicted output is 't' the last character in the sequence before it moves to the next sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGg-pcedsFns",
        "colab_type": "code",
        "outputId": "09f35d9c-8b66-4e88-a6f1-559c2e58a9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 30 ('<')\n",
            "  expected output: 3 ('!')\n",
            "Step    1\n",
            "  input: 3 ('!')\n",
            "  expected output: 38 ('D')\n",
            "Step    2\n",
            "  input: 38 ('D')\n",
            "  expected output: 49 ('O')\n",
            "Step    3\n",
            "  input: 49 ('O')\n",
            "  expected output: 37 ('C')\n",
            "Step    4\n",
            "  input: 37 ('C')\n",
            "  expected output: 54 ('T')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hotbjJ7fv1NY",
        "colab_type": "code",
        "outputId": "ffad6508-1dba-4c01-fce1-7108cc9bc049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 107\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((107, 50), (107, 50)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diyWFjQnv1ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Batch size is usually the size of your vocabulary because you will want to maintain a reasonable proportional embedding distance between your vocab\n",
        "\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY8Aig6U4Ls4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdJPZIYW4RiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrALEylv4gzf",
        "colab_type": "code",
        "outputId": "a351f605-cd71-4379-db91-08910b08754a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(107, 50, 108) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPt6pdoJ6d7o",
        "colab_type": "code",
        "outputId": "51f463f7-5c64-4c37-c588-caf30d000f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (107, None, 256)          27648     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (107, None, 1024)         3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (107, None, 108)          110700    \n",
            "=================================================================\n",
            "Total params: 4,076,652\n",
            "Trainable params: 4,076,652\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfq7PJa97pc9",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5nFttwc7rBg",
        "colab_type": "code",
        "outputId": "5658637c-5c9e-45b7-c757-b73cd2dc1722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 34,  92, 100,  91,  59,  85,   5,  13,  66, 106,  84, 104,  24,\n",
              "        20,  17,  11,  87,  77,  12,  69,  95,  40, 102,  49,  62, 101,\n",
              "        56,   9,  21,  68,  60,  83,  77,  56,   1,  20,  17,   2,  18,\n",
              "        95,  39,  68,  58,  24,  55,  54,  95,  63,  21,  79])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE2glKqk8l5p",
        "colab_type": "text"
      },
      "source": [
        "Checking the sample text generated by this untrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD5YeymL765X",
        "colab_type": "code",
        "outputId": "79b88c34-74b1-4a1e-831a-b97473088c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'e comment, delete the comment or reply to the comm'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"@{’zYt#+a└s─62/)vl*d¥F€O\\\\…V'3cZrlV\\n2/ 0¥EcX6UT¥]3n\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M1ooepH87Tq",
        "colab_type": "text"
      },
      "source": [
        "Attach an optimizer, and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcFIrNua8s0a",
        "colab_type": "code",
        "outputId": "40998b72-4848-4593-97ed-de239be4c588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (107, 50, 108)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.683313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkkvXbaX_wN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClVv7IPhDUqo",
        "colab_type": "text"
      },
      "source": [
        "Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek1oJajA_xB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaegGaZ3FVeU",
        "colab_type": "code",
        "outputId": "aaad6c0c-7c02-44a4-9761-9e91a9571903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "509/509 [==============================] - 1930s 4s/step - loss: 1.5939\n",
            "Epoch 2/10\n",
            "509/509 [==============================] - 1942s 4s/step - loss: 0.7561\n",
            "Epoch 3/10\n",
            "509/509 [==============================] - 1897s 4s/step - loss: 0.5748\n",
            "Epoch 4/10\n",
            "509/509 [==============================] - 1987s 4s/step - loss: 0.5004\n",
            "Epoch 5/10\n",
            "509/509 [==============================] - 1966s 4s/step - loss: 0.4588\n",
            "Epoch 6/10\n",
            "509/509 [==============================] - 1802s 4s/step - loss: 0.4381\n",
            "Epoch 7/10\n",
            "509/509 [==============================] - 1889s 4s/step - loss: 0.4142\n",
            "Epoch 8/10\n",
            "509/509 [==============================] - 1862s 4s/step - loss: 0.3976\n",
            "Epoch 9/10\n",
            "509/509 [==============================] - 1897s 4s/step - loss: 0.3862\n",
            "Epoch 10/10\n",
            "509/509 [==============================] - 1932s 4s/step - loss: 0.3768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN-dX1FpjZz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8eb0ed65-bb3e-4fec-a8d6-612c0aee0b1d"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhZNnkokE43z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3vUrZ-1QgzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "22d747b5-8733-46d3-bff4-bb711b27e3ba"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (1, None, 256)            27648     \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (1, None, 108)            110700    \n",
            "=================================================================\n",
            "Total params: 4,076,652\n",
            "Trainable params: 4,076,652\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBoM5iEDQy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.5\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3KL_pKSREVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "4b4ede2f-edf5-49de-e09a-d05906021dae"
      },
      "source": [
        "print(generate_text(model, start_string=u\"<!DOCTYPE html> \"))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html> <html lang=\"en\">\n",
            "\n",
            "<head>\n",
            "  <meta charset=\"utf-8\">\n",
            "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "    \n",
            "                                                                                                    <h3><i class=\"m-icon-big-swapright m-icon-white\"></i> Pages</a>\n",
            "                            </div>\n",
            "                                                                                                                              <li><a href=\"#\"><i class=\"icon-picture\"></i> All category</a></li>\n",
            "                      </ul>\n",
            "                                      <h2 class=\"widget-title\">Instagram Feed</h2>\n",
            "                                                                      </div>\n",
            "                                                                                                                                                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}